{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e5625f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torchaudio\n",
    "from fastai.vision.all import *\n",
    "from fastai.callback.all import *\n",
    "from fastai.data.all import *\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4047062",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run model.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebf81c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run loss.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7afe486",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run metrics.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5836c1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import torch_directml\n",
    "    dml = torch_directml.device()\n",
    "    print(f\"DirectML device available: {dml} | {torch_directml.device_name(0)}\")\n",
    "    USE_DIRECTML = True\n",
    "except ImportError:\n",
    "    print(\"torch_directml not available, using CPU\")\n",
    "    USE_DIRECTML = False\n",
    "    dml = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3447782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioTensor(TensorBase):\n",
    "    \"\"\"Wrapper for audio tensors\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def load_audio(path_str, target_length=64000):\n",
    "    \"\"\"Load and preprocess audio file\"\"\"\n",
    "    path_str = Path(path_str)\n",
    "    wave, sr = torchaudio.load(str(path_str))\n",
    "    \n",
    "    # Convert to mono\n",
    "    if wave.shape[0] > 1:\n",
    "        wave = wave.mean(0, keepdim=True)\n",
    "    \n",
    "    # Resample if needed\n",
    "    if sr != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(sr, 16000)\n",
    "        wave = resampler(wave)\n",
    "    \n",
    "    # Pad or crop to target length\n",
    "    current_length = wave.shape[1]\n",
    "    if current_length < target_length:\n",
    "        padding = target_length - current_length\n",
    "        wave = F.pad(wave, (0, padding))\n",
    "    else:\n",
    "        wave = wave[:, :target_length]\n",
    "    \n",
    "    return AudioTensor(wave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bb8b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataloaders(noisy_dir, clean_dir, bs=8, valid_pct=0.15, verbose=False,\n",
    "                              target_length=64000, num_workers=0, device=torch.device(\"cpu\")):\n",
    "    \"\"\"\n",
    "    Create DataLoaders for VoiceBank-DEMAND dataset\n",
    "    \n",
    "    Args:\n",
    "        noisy_dir: Path to noisy audio files\n",
    "        clean_dir: Path to clean audio files\n",
    "        bs: Batch size\n",
    "        valid_pct: Validation split percentage\n",
    "        target_length: Fixed audio length in samples (64000 = 4 seconds @ 16kHz)\n",
    "        num_workers: Number of data loading workers\n",
    "    \"\"\"\n",
    "    noisy_dir = Path(noisy_dir)\n",
    "    clean_dir = Path(clean_dir)\n",
    "    \n",
    "    # Get all noisy files\n",
    "    noisy_files = sorted(list(noisy_dir.glob('*.wav')))\n",
    "    \n",
    "    # Create pairs by matching filenames\n",
    "    items = [str(noisy_file) for noisy_file in noisy_files \n",
    "             if (clean_dir / noisy_file.name).exists()]\n",
    "    \n",
    "    print(f\"Found {len(items)} audio pairs\")\n",
    "    \n",
    "    def get_x(noisy_audio_path):\n",
    "        return load_audio(noisy_audio_path, target_length)\n",
    "    \n",
    "    def get_y(noisy_audio_path):\n",
    "        noisy_path = Path(noisy_audio_path)\n",
    "        clean_path = clean_dir / noisy_path.name\n",
    "        return load_audio(str(clean_path), target_length)\n",
    "    \n",
    "    # Custom type dispatch for AudioTensor\n",
    "    def AudioTensorBlock():\n",
    "        return TransformBlock(type_tfms=[], batch_tfms=[])\n",
    "    \n",
    "    dblock = DataBlock(\n",
    "        blocks=(AudioTensorBlock(), AudioTensorBlock()),\n",
    "        get_x=get_x,\n",
    "        get_y=get_y,\n",
    "        splitter=RandomSplitter(valid_pct=valid_pct, seed=42)\n",
    "    )\n",
    "    \n",
    "    dls = dblock.dataloaders(items, bs=bs, num_workers=num_workers, verbose=verbose)\n",
    "    dls = dls.to(dml)\n",
    "\n",
    "    return dls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fbaab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_learner(\n",
    "    train_noisy_dir=\"data/train/noisy_trainset_28spk_wav\",\n",
    "    train_clean_dir=\"data/train/clean_trainset_28spk_wav\",\n",
    "    epochs=80,\n",
    "    lr=3e-4,\n",
    "    batch_size=8,\n",
    "    channels=96,\n",
    "    num_blocks=4,\n",
    "    device=torch.device(\"cpu\"),\n",
    "    verbose=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the causal noise removal model\n",
    "    \n",
    "    Args:\n",
    "        train_noisy_dir: Path to noisy training audio\n",
    "        train_clean_dir: Path to clean training audio\n",
    "        epochs: Number of training epochs\n",
    "        lr: Learning rate\n",
    "        batch_size: Batch size\n",
    "        channels: Number of channels in model\n",
    "        num_blocks: Number of processing blocks\n",
    "        use_56spk: Use 56 speaker dataset instead of 28\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Loading data from:\")\n",
    "    print(f\"Noisy: {train_noisy_dir}\")\n",
    "    print(f\"Clean: {train_clean_dir}\")\n",
    "    \n",
    "    # Create dataloaders\n",
    "    dls = generate_dataloaders(\n",
    "        train_noisy_dir, \n",
    "        train_clean_dir,\n",
    "        target_length=80000,\n",
    "        bs=batch_size,\n",
    "        valid_pct=0.1,\n",
    "        device=device,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    \n",
    "    # Show a batch to verify\n",
    "    print(\"\\nDataLoader check:\")\n",
    "    xb, yb = dls.one_batch()\n",
    "    print(f\"  Noisy batch shape: {xb.shape}\")\n",
    "    print(f\"  Clean batch shape: {yb.shape}\")\n",
    "    \n",
    "    # Create model\n",
    "    model = CausalDNoizeConvTasNet(channels=channels, num_blocks=num_blocks)\n",
    "    \n",
    "    # Move to DirectML device if available\n",
    "    if USE_DIRECTML:\n",
    "        model = model.to(dml)\n",
    "        print(f\"\\nModel moved to DirectML device\")\n",
    "    \n",
    "    # Create learner\n",
    "    learn = Learner(\n",
    "        dls,\n",
    "        model,\n",
    "        loss_func=CombinedLoss(),\n",
    "        opt_func=Adam,\n",
    "        metrics=[pesq_metric, stoi_metric, DenoisingAccuracy()],\n",
    "        cbs=[\n",
    "            SaveModelCallback(\n",
    "                monitor='accuracy_%',\n",
    "                fname='causal_dnoize_best'\n",
    "            )\n",
    "        ]\n",
    "    ).to_fp16(enabled=False)\n",
    "    \n",
    "    # Override device if using DirectML\n",
    "    if USE_DIRECTML:\n",
    "        learn.dls.device = device\n",
    "        learn.model = learn.model.to(device)\n",
    "    \n",
    "    print(f\"  Batch size: {batch_size}\")\n",
    "    print(f\"  Model channels: {channels}\")\n",
    "    print(f\"  Model blocks: {num_blocks}\")\n",
    "    \n",
    "    return learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c5a51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = generate_learner(\n",
    "    train_noisy_dir=\"data/train/noisy_trainset_28spk_wav\",\n",
    "    train_clean_dir=\"data/train/clean_trainset_28spk_wav\",\n",
    "    batch_size=4,\n",
    "    channels=128,\n",
    "    num_blocks=6,\n",
    "    device=dml\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00f46d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd075ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "n_epochs = 150\n",
    "lr = 0.000575\n",
    "best_pesq = 0\n",
    "\n",
    "print(\"Starting training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0974107d",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(n_epochs, lr_max=lr, div=25, pct_start=0.3, wd=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cc1c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('causal_dnoize_final')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

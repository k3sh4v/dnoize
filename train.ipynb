{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d7885f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"3\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"3\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"3\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e5625f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torchaudio\n",
    "from fastai.vision.all import *\n",
    "from fastai.callback.all import *\n",
    "from fastai.data.all import *\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4047062",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run model.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebf81c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run loss.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7afe486",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run metrics.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1048c839",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run helpers.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5836c1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import torch_directml\n",
    "    dml = torch_directml.device()\n",
    "    print(f\"DirectML device available: {dml} | {torch_directml.device_name(0)}\")\n",
    "    USE_DIRECTML = True\n",
    "except ImportError:\n",
    "    print(\"torch_directml not available, using CPU\")\n",
    "    USE_DIRECTML = False\n",
    "    dml = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3447782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioTensor(TensorBase):\n",
    "    \"\"\"Wrapper for audio tensors\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def load_audio(path_str, target_length=64000):\n",
    "    \"\"\"Load and preprocess audio file\"\"\"\n",
    "    path_str = Path(path_str)\n",
    "    wave, sr = torchaudio.load(str(path_str))\n",
    "    \n",
    "    # Convert to mono\n",
    "    if wave.shape[0] > 1:\n",
    "        wave = wave.mean(0, keepdim=True)\n",
    "    \n",
    "    # Resample if needed\n",
    "    if sr != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(sr, 16000)\n",
    "        wave = resampler(wave)\n",
    "    \n",
    "    # Pad or crop to target length\n",
    "    current_length = wave.shape[1]\n",
    "    if current_length < target_length:\n",
    "        padding = target_length - current_length\n",
    "        wave = F.pad(wave, (0, padding))\n",
    "    else:\n",
    "        wave = wave[:, :target_length]\n",
    "    \n",
    "    return AudioTensor(wave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bb8b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataloaders(noisy_dir, clean_dir, bs=8, valid_pct=0.15, verbose=False,\n",
    "                              target_length=64000, num_workers=0, device=torch.device(\"cpu\")):\n",
    "    \"\"\"\n",
    "    Create DataLoaders for VoiceBank-DEMAND dataset\n",
    "    \n",
    "    Args:\n",
    "        noisy_dir: Path to noisy audio files\n",
    "        clean_dir: Path to clean audio files\n",
    "        bs: Batch size\n",
    "        valid_pct: Validation split percentage\n",
    "        target_length: Fixed audio length in samples (64000 = 4 seconds @ 16kHz)\n",
    "        num_workers: Number of data loading workers\n",
    "    \"\"\"\n",
    "    noisy_dir = Path(noisy_dir)\n",
    "    clean_dir = Path(clean_dir)\n",
    "    \n",
    "    # Get all noisy files\n",
    "    noisy_files = sorted(list(noisy_dir.glob('*.wav')))\n",
    "    \n",
    "    # Create pairs by matching filenames\n",
    "    items = [str(noisy_file) for noisy_file in noisy_files \n",
    "             if (clean_dir / noisy_file.name).exists()]\n",
    "    \n",
    "    print(f\"Found {len(items)} audio pairs\")\n",
    "    \n",
    "    def get_x(noisy_audio_path):\n",
    "        return load_audio(noisy_audio_path, target_length)\n",
    "    \n",
    "    def get_y(noisy_audio_path):\n",
    "        noisy_path = Path(noisy_audio_path)\n",
    "        clean_path = clean_dir / noisy_path.name\n",
    "        return load_audio(str(clean_path), target_length)\n",
    "    \n",
    "    # Custom type dispatch for AudioTensor\n",
    "    def AudioTensorBlock():\n",
    "        return TransformBlock(type_tfms=[], batch_tfms=[])\n",
    "    \n",
    "    dblock = DataBlock(\n",
    "        blocks=(AudioTensorBlock(), AudioTensorBlock()),\n",
    "        get_x=get_x,\n",
    "        get_y=get_y,\n",
    "        splitter=RandomSplitter(valid_pct=valid_pct, seed=42)\n",
    "    )\n",
    "    \n",
    "    dls = dblock.dataloaders(items, bs=bs, num_workers=num_workers, verbose=verbose)\n",
    "    dls = dls.to(dml)\n",
    "\n",
    "    return dls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fbaab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_learner(\n",
    "    train_noisy_dir=\"data/train/noisy_trainset_28spk_wav\",\n",
    "    train_clean_dir=\"data/train/clean_trainset_28spk_wav\",\n",
    "    epochs=80,\n",
    "    batch_size=8,\n",
    "    channels=96,\n",
    "    num_blocks=10,\n",
    "    num_repeats=2,\n",
    "    target_length=64000,\n",
    "    valid_pct=0.05,\n",
    "    device=torch.device(\"cpu\"),\n",
    "    verbose=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the causal noise removal model\n",
    "    \n",
    "    Args:\n",
    "        train_noisy_dir: Path to noisy training audio\n",
    "        train_clean_dir: Path to clean training audio\n",
    "        epochs: Number of training epochs\n",
    "        batch_size: Batch size\n",
    "        channels: Number of channels in model\n",
    "        num_blocks: Number of processing blocks\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Loading data from:\")\n",
    "    print(f\"Noisy: {train_noisy_dir}\")\n",
    "    print(f\"Clean: {train_clean_dir}\")\n",
    "    \n",
    "    # Create dataloaders\n",
    "    dls = generate_dataloaders(\n",
    "        train_noisy_dir, \n",
    "        train_clean_dir,\n",
    "        target_length=target_length,\n",
    "        bs=batch_size,\n",
    "        valid_pct=valid_pct,\n",
    "        device=device,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    \n",
    "    # Show a batch to verify\n",
    "    print(\"\\nDataLoader check:\")\n",
    "    xb, yb = dls.one_batch()\n",
    "    print(f\"  Noisy batch shape: {xb.shape}\")\n",
    "    print(f\"  Clean batch shape: {yb.shape}\")\n",
    "    \n",
    "    # Create model\n",
    "    model = CausalDNoizeConvTasNet(\n",
    "        channels=channels, num_blocks=num_blocks,\n",
    "        num_repeats=num_repeats\n",
    "    )\n",
    "    \n",
    "    # Move to DirectML device if available\n",
    "    if USE_DIRECTML:\n",
    "        model = model.to(dml)\n",
    "        print(f\"\\nModel moved to DirectML device\")\n",
    "    \n",
    "    # Create learner\n",
    "    learn = Learner(\n",
    "        dls,\n",
    "        model,\n",
    "        loss_func=CombinedLoss(use_spectral=True),\n",
    "        opt_func=lookahead_adamw,\n",
    "        metrics=[SISNRMetric(), NoiseReductionPct()],\n",
    "        cbs=[\n",
    "            SaveModelCallback(monitor='sisnr_db', fname='causal_dnoize_best', with_opt=True),\n",
    "            GradientClip(max_norm=1.0), GradientAccumulation(n_acc=4), SISNRDiagnostic()\n",
    "        ]\n",
    "    ).to_fp16(enabled=False)\n",
    "    \n",
    "    # # Override device if using DirectML\n",
    "    # if USE_DIRECTML:\n",
    "    #     learn.model = learn.model.to(device)\n",
    "    \n",
    "    print(f\"  Batch size: {batch_size}\")\n",
    "    print(f\"  Model channels: {channels}\")\n",
    "    print(f\"  Model blocks: {num_blocks}\")\n",
    "    \n",
    "    return learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c5a51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "learn = generate_learner(\n",
    "    train_noisy_dir=\"data/train/noisy_trainset_28spk_wav\",\n",
    "    train_clean_dir=\"data/train/clean_trainset_28spk_wav\",\n",
    "    batch_size=4,\n",
    "    channels=48,\n",
    "    num_blocks=8,\n",
    "    num_repeats=2,\n",
    "    target_length=48000,\n",
    "    valid_pct=0.05,\n",
    "    device=dml\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203681f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On resume â€” automatically knows how many epochs are left\n",
    "n_epochs = 200\n",
    "tracker = EpochTracker(total_epochs=n_epochs)\n",
    "epochs_remaining = 200 - tracker.epochs_done\n",
    "\n",
    "resuming_after_failure = tracker.epochs_done > 0\n",
    "\n",
    "if resuming_after_failure:\n",
    "    print(f\"Resuming from epoch {tracker.epochs_done}, {epochs_remaining} remaining\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ff280c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best checkpoint saved so far on training failure\n",
    "if resuming_after_failure:\n",
    "    learn.load('causal_dnoize_best', with_opt=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00f46d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not resuming_after_failure:\n",
    "    suggested_lr = learn.lr_find(stop_div=False, suggest_funcs=(steep, valley))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f434136",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not resuming_after_failure:\n",
    "    print(suggested_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e5717f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if resuming_after_failure:\n",
    "    # Reduce LR proportionally to training progress\n",
    "    lr_max = tracker.saved_lr_max\n",
    "    progress  = tracker.epochs_done / tracker.total_epochs\n",
    "\n",
    "    resume_lr = lr_max * max(0.1, 1 - progress * 0.8)\n",
    "    \n",
    "    print(f\"Resume LR: {resume_lr} | Saved Max LR: {lr_max}\")\n",
    "    \n",
    "else:\n",
    "    # Use valley directly if steep is unreasonably small\n",
    "    if suggested_lr.steep < 1e-5:\n",
    "        lr_max = suggested_lr.valley / 10.\n",
    "        print(f\"steep too small, using valley / 10: {lr_max:.8f}\")\n",
    "    else:\n",
    "        lr_max = math.exp(\n",
    "            (math.log(suggested_lr.steep) + math.log(suggested_lr.valley)) / 2\n",
    "        )\n",
    "        print(f\"using geometric mean: {lr_max:.8f}\")\n",
    "    tracker.saved_lr_max = lr_max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd075ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.add_cb(tracker)\n",
    "\n",
    "training_start_msg = (\n",
    "    f\"Starting training for {epochs_remaining} epochs with {\"lr_max = \"+str(lr_max) if not resuming_after_failure else \"lr = \"+str(resume_lr)} | \"\n",
    "    f\"{'fresh start' if not resuming_after_failure else 'resuming after '+str(tracker.epochs_done)+' epochs'}\"\n",
    ")\n",
    "\n",
    "print(training_start_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0974107d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if resuming_after_failure:\n",
    "    # no flat phase, pure cosine decay from start\n",
    "    learn.fit_flat_cos(epochs_remaining, lr=resume_lr, pct_start=0.0, wd=1e-4)\n",
    "else:\n",
    "    learn.fit_one_cycle(epochs_remaining, lr_max=lr_max, div=25, pct_start=0.05, wd=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cc1c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('causal_dnoize_final', with_opt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c997d731",
   "metadata": {},
   "outputs": [],
   "source": [
    "pesq_score, stoi_score = evaluate_checkpoint(\n",
    "    'models/causal_dnoize_best.pth', learn.dls\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03de4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pesq_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edeb7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

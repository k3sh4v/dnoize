{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bab2e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "from datetime import timedelta\n",
    "from fastai.callback.all import *\n",
    "from fastai.optimizer import Lookahead, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9d59e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookahead_adamw(params, **kwargs):\n",
    "    kwargs['decouple_wd'] = True\n",
    "    return Lookahead(Adam(params, **kwargs), k=6, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485742fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpochTracker(Callback):\n",
    "    \"\"\"\n",
    "    Tracks epoch count, lr_max and per-epoch metrics.\n",
    "    Saves to disk after every epoch for clean resume after failure.\n",
    "    \n",
    "    Files:\n",
    "      tracker_fname — epochs_done, lr_max (for resume)\n",
    "      log_fname     — full per-epoch metrics CSV\n",
    "    \n",
    "    Order=60 ensures this runs after Recorder(order=50) so\n",
    "    recorder.values[-1] is fully populated when after_epoch fires.\n",
    "    \"\"\"\n",
    "    order = 60\n",
    "\n",
    "    def __init__(self, lr_max=0, total_epochs=200,\n",
    "                 tracker_fname='models/epoch_tracker.txt',\n",
    "                 log_fname='models/training_log.csv'):\n",
    "        self.saved_lr_max  = lr_max\n",
    "        self.total_epochs  = total_epochs\n",
    "        self.tracker_fname = tracker_fname\n",
    "        self.log_fname     = log_fname\n",
    "        self.epochs_done, self.saved_lr_max = self._load_tracker()\n",
    "        self._init_log()\n",
    "\n",
    "    def _load_tracker(self):\n",
    "        try:\n",
    "            with open(self.tracker_fname, 'r') as f:\n",
    "                parts = f.read().strip().split(',')\n",
    "                epochs = int(parts[0])\n",
    "                lr_max = float(parts[1]) if len(parts) > 1 else self.saved_lr_max\n",
    "                return epochs, lr_max\n",
    "        except:\n",
    "            return 0, self.saved_lr_max\n",
    "\n",
    "    def _save_tracker(self):\n",
    "        os.makedirs(os.path.dirname(self.tracker_fname), exist_ok=True)\n",
    "        with open(self.tracker_fname, 'w') as f:\n",
    "            f.write(f\"{self.epochs_done},{self.saved_lr_max}\")\n",
    "\n",
    "    def _init_log(self):\n",
    "        os.makedirs(os.path.dirname(self.log_fname), exist_ok=True)\n",
    "        if not Path(self.log_fname).exists():\n",
    "            with open(self.log_fname, 'w', newline='') as f:\n",
    "                csv.writer(f).writerow([\n",
    "                    'epoch',\n",
    "                    'train_loss',\n",
    "                    'valid_loss',\n",
    "                    'sisnr_db',\n",
    "                    'noise_removed_percentage',\n",
    "                    'current_lr',\n",
    "                    'lr_max',\n",
    "                    'epoch_elapsed_time'\n",
    "                ])\n",
    "\n",
    "    def before_epoch(self):\n",
    "        self._epoch_start = time.time()\n",
    "\n",
    "    def after_epoch(self):\n",
    "        \"\"\"\n",
    "        Runs after Recorder.after_epoch (order=50) so recorder.values[-1]\n",
    "        is fully populated with [train_loss, valid_loss, sisnr_db, noise_removed_%]\n",
    "        \"\"\"\n",
    "        elapsed  = time.time() - self._epoch_start\n",
    "        self.epochs_done += 1\n",
    "\n",
    "        recorder = self.learn.recorder\n",
    "\n",
    "        # recorder.values[-1] layout (confirmed from debug output):\n",
    "        # [train_loss, valid_loss, sisnr_db, noise_removed_%]\n",
    "        vals = recorder.values[-1] if recorder.values else []\n",
    "\n",
    "        train_loss    = vals[0] if len(vals) > 0 else None\n",
    "        valid_loss    = vals[1] if len(vals) > 1 else None\n",
    "        sisnr_db      = vals[2] if len(vals) > 2 else None\n",
    "        noise_removed = vals[3] if len(vals) > 3 else None\n",
    "\n",
    "        current_lr = self.learn.opt.hypers[-1]['lr'] if self.learn.opt else None\n",
    "\n",
    "        with open(self.log_fname, 'a', newline='') as f:\n",
    "            csv.writer(f).writerow([\n",
    "                self.epochs_done,\n",
    "                f\"{train_loss:.6f}\"    if train_loss     is not None else '',\n",
    "                f\"{valid_loss:.6f}\"    if valid_loss     is not None else '',\n",
    "                f\"{sisnr_db:.6f}\"      if sisnr_db       is not None else '',\n",
    "                f\"{noise_removed:.6f}\" if noise_removed  is not None else '',\n",
    "                f\"{current_lr}\"        if current_lr     is not None else '',\n",
    "                f\"{self.saved_lr_max}\",\n",
    "                f\"{timedelta(seconds=elapsed)}\"\n",
    "            ])\n",
    "\n",
    "        self._save_tracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d0d652",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SISNRDiagnostic(Callback):\n",
    "    \"\"\"\n",
    "    Computes manual SI-SNR on validation batch after each epoch\n",
    "    and writes to file for monitoring without interrupting training.\n",
    "    \"\"\"\n",
    "    def __init__(self, fname='models/sisnr_diagnostic.csv', every_n=1):\n",
    "        self.fname   = fname\n",
    "        self.every_n = every_n\n",
    "        self._init_file()\n",
    "\n",
    "    def _init_file(self):\n",
    "        os.makedirs(os.path.dirname(self.fname), exist_ok=True)\n",
    "        if not Path(self.fname).exists():\n",
    "            with open(self.fname, 'w', newline='') as f:\n",
    "                csv.writer(f).writerow([\n",
    "                    'epoch',\n",
    "                    'sisnr_sample_0', 'sisnr_sample_1',\n",
    "                    'sisnr_sample_2', 'sisnr_sample_3',\n",
    "                    'sisnr_mean',\n",
    "                    'pred_min', 'pred_max',\n",
    "                    'pred_power_mean', 'targ_power_mean',\n",
    "                    'power_ratio'\n",
    "                ])\n",
    "\n",
    "    def after_epoch(self):\n",
    "        if self.epoch % self.every_n != 0:\n",
    "            return\n",
    "        try:\n",
    "            self.learn.model.eval()\n",
    "            xb, yb = self.learn.dls.valid.one_batch()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                pred = self.learn.model(xb)\n",
    "\n",
    "            pred_sq = pred.squeeze(1).detach().float()\n",
    "            targ_sq = yb.squeeze(1).detach().float()\n",
    "\n",
    "            # Manual SI-SNR\n",
    "            targ_zm = targ_sq - targ_sq.mean(dim=-1, keepdim=True)\n",
    "            pred_zm = pred_sq - pred_sq.mean(dim=-1, keepdim=True)\n",
    "\n",
    "            eps = 1e-8\n",
    "            alpha = (targ_zm * pred_zm).sum(-1, keepdim=True) / \\\n",
    "                    (targ_zm.pow(2).sum(-1, keepdim=True) + eps)\n",
    "            s = (alpha * targ_zm).pow(2).sum(-1)\n",
    "            n = (pred_zm - alpha * targ_zm).pow(2).sum(-1)\n",
    "            sisnr = 10 * torch.log10((s + eps) / (n + eps))\n",
    "\n",
    "            # Power ratio — should trend toward 1.0 as training progresses\n",
    "            pred_power = pred_zm.pow(2).sum(dim=-1).mean().item()\n",
    "            targ_power = targ_zm.pow(2).sum(dim=-1).mean().item()\n",
    "            ratio      = pred_power / (targ_power + eps)\n",
    "\n",
    "            samples = sisnr.cpu().tolist()\n",
    "            # Pad to 4 samples if batch is smaller\n",
    "            while len(samples) < 4:\n",
    "                samples.append('')\n",
    "\n",
    "            with open(self.fname, 'a', newline='') as f:\n",
    "                csv.writer(f).writerow([\n",
    "                    self.epoch + 1,\n",
    "                    f\"{samples[0]:.4f}\" if samples[0] != '' else '',\n",
    "                    f\"{samples[1]:.4f}\" if samples[1] != '' else '',\n",
    "                    f\"{samples[2]:.4f}\" if samples[2] != '' else '',\n",
    "                    f\"{samples[3]:.4f}\" if samples[3] != '' else '',\n",
    "                    f\"{sisnr.mean().item():.4f}\",\n",
    "                    f\"{pred_sq.min().item():.4f}\",\n",
    "                    f\"{pred_sq.max().item():.4f}\",\n",
    "                    f\"{pred_power:.4f}\",\n",
    "                    f\"{targ_power:.4f}\",\n",
    "                    f\"{ratio:.4f}\"\n",
    "                ])\n",
    "\n",
    "        except Exception as e:\n",
    "            # Never crash training — just log the error\n",
    "            with open(self.fname, 'a', newline='') as f:\n",
    "                csv.writer(f).writerow([self.epoch + 1, f\"ERROR: {e}\",\n",
    "                                        '', '', '', '', '', '', '', '', ''])\n",
    "        finally:\n",
    "            self.learn.model.train()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

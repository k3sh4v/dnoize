{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bab2e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import csv\n",
    "import pyroomacoustics as pra\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "import torchaudio\n",
    "from fastai.callback.all import *\n",
    "from fastai.optimizer import Lookahead, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9d59e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookahead_adamw(params, **kwargs):\n",
    "    kwargs['decouple_wd'] = True\n",
    "    return Lookahead(Adam(params, **kwargs), k=6, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485742fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpochTracker(Callback):\n",
    "    \"\"\"\n",
    "    Tracks epoch count, lr_max and per-epoch metrics.\n",
    "    Saves to disk after every epoch for clean resume after failure.\n",
    "    \n",
    "    Files:\n",
    "      tracker_fname — epochs_done, lr_max (for resume)\n",
    "      log_fname     — full per-epoch metrics CSV\n",
    "    \n",
    "    Order=60 ensures this runs after Recorder(order=50) so\n",
    "    recorder.values[-1] is fully populated when after_epoch fires.\n",
    "    \"\"\"\n",
    "    order = 60\n",
    "\n",
    "    def __init__(self, lr_max=0, total_epochs=200,\n",
    "                 tracker_fname='training_stats/epoch_tracker.txt',\n",
    "                 log_fname='training_stats/training_log.csv'):\n",
    "        self.saved_lr_max  = lr_max\n",
    "        self.total_epochs  = total_epochs\n",
    "        self.tracker_fname = tracker_fname\n",
    "        self.log_fname     = log_fname\n",
    "        self.epochs_done, self.saved_lr_max = self._load_tracker()\n",
    "        self._init_log()\n",
    "\n",
    "    def _load_tracker(self):\n",
    "        try:\n",
    "            with open(self.tracker_fname, 'r') as f:\n",
    "                parts = f.read().strip().split(',')\n",
    "                epochs = int(parts[0])\n",
    "                lr_max = float(parts[1]) if len(parts) > 1 else self.saved_lr_max\n",
    "                return epochs, lr_max\n",
    "        except:\n",
    "            return 0, self.saved_lr_max\n",
    "\n",
    "    def _save_tracker(self):\n",
    "        os.makedirs(os.path.dirname(self.tracker_fname), exist_ok=True)\n",
    "        with open(self.tracker_fname, 'w') as f:\n",
    "            f.write(f\"{self.epochs_done},{self.saved_lr_max}\")\n",
    "\n",
    "    def _init_log(self):\n",
    "        os.makedirs(os.path.dirname(self.log_fname), exist_ok=True)\n",
    "        if not Path(self.log_fname).exists():\n",
    "            with open(self.log_fname, 'w', newline='') as f:\n",
    "                csv.writer(f).writerow([\n",
    "                    'epoch',\n",
    "                    'train_loss',\n",
    "                    'valid_loss',\n",
    "                    'sisnr_db',\n",
    "                    'noise_removed_percentage',\n",
    "                    'current_lr',\n",
    "                    'lr_max',\n",
    "                    'epoch_elapsed_time'\n",
    "                ])\n",
    "\n",
    "    def before_epoch(self):\n",
    "        self._epoch_start = time.time()\n",
    "        set_epoch_seed(self.epochs_done)\n",
    "\n",
    "    def after_epoch(self):\n",
    "        \"\"\"\n",
    "        Runs after Recorder.after_epoch (order=50) so recorder.values[-1]\n",
    "        is fully populated with [train_loss, valid_loss, sisnr_db, noise_removed_%]\n",
    "        \"\"\"\n",
    "        elapsed  = time.time() - self._epoch_start\n",
    "        self.epochs_done += 1\n",
    "\n",
    "        recorder = self.learn.recorder\n",
    "\n",
    "        # recorder.values[-1] layout (confirmed from debug output):\n",
    "        # [train_loss, valid_loss, sisnr_db, noise_removed_%]\n",
    "        vals = recorder.values[-1] if recorder.values else []\n",
    "\n",
    "        train_loss    = vals[0] if len(vals) > 0 else None\n",
    "        valid_loss    = vals[1] if len(vals) > 1 else None\n",
    "        sisnr_db      = vals[2] if len(vals) > 2 else None\n",
    "        noise_removed = vals[3] if len(vals) > 3 else None\n",
    "\n",
    "        current_lr = self.learn.opt.hypers[-1]['lr'] if self.learn.opt else None\n",
    "\n",
    "        with open(self.log_fname, 'a', newline='') as f:\n",
    "            csv.writer(f).writerow([\n",
    "                self.epochs_done,\n",
    "                f\"{train_loss:.6f}\"    if train_loss     is not None else '',\n",
    "                f\"{valid_loss:.6f}\"    if valid_loss     is not None else '',\n",
    "                f\"{sisnr_db:.6f}\"      if sisnr_db       is not None else '',\n",
    "                f\"{noise_removed:.6f}\" if noise_removed  is not None else '',\n",
    "                f\"{current_lr}\"        if current_lr     is not None else '',\n",
    "                f\"{self.saved_lr_max}\",\n",
    "                f\"{timedelta(seconds=elapsed)}\"\n",
    "            ])\n",
    "\n",
    "        self._save_tracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d0d652",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SISNRDiagnostic(Callback):\n",
    "    \"\"\"\n",
    "    Computes manual SI-SNR on validation batch after each epoch\n",
    "    and writes to file for monitoring without interrupting training.\n",
    "    \"\"\"\n",
    "    def __init__(self, fname='training_stats/sisnr_diagnostic.csv', every_n=1):\n",
    "        self.fname   = fname\n",
    "        self.every_n = every_n\n",
    "        self._init_file()\n",
    "\n",
    "    def _init_file(self):\n",
    "        os.makedirs(os.path.dirname(self.fname), exist_ok=True)\n",
    "        if not Path(self.fname).exists():\n",
    "            with open(self.fname, 'w', newline='') as f:\n",
    "                csv.writer(f).writerow([\n",
    "                    'epoch',\n",
    "                    'sisnr_sample_0', 'sisnr_sample_1',\n",
    "                    'sisnr_sample_2', 'sisnr_sample_3',\n",
    "                    'sisnr_mean',\n",
    "                    'pred_min', 'pred_max',\n",
    "                    'pred_power_mean', 'targ_power_mean',\n",
    "                    'power_ratio'\n",
    "                ])\n",
    "\n",
    "    def after_epoch(self):\n",
    "        if self.epoch % self.every_n != 0:\n",
    "            return\n",
    "        try:\n",
    "            self.learn.model.eval()\n",
    "            xb, yb = self.learn.dls.valid.one_batch()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                pred = self.learn.model(xb)\n",
    "\n",
    "            pred_sq = pred.squeeze(1).detach().float().cpu()\n",
    "            targ_sq = yb.squeeze(1).detach().float().cpu()\n",
    "\n",
    "            targ_zm = targ_sq - targ_sq.mean(dim=-1, keepdim=True)\n",
    "            pred_zm = pred_sq - pred_sq.mean(dim=-1, keepdim=True)\n",
    "\n",
    "            eps = 1e-8\n",
    "            alpha = (targ_zm * pred_zm).sum(-1, keepdim=True) / \\\n",
    "                    (targ_zm.pow(2).sum(-1, keepdim=True) + eps)\n",
    "            s = (alpha * targ_zm).pow(2).sum(-1)\n",
    "            n = (pred_zm - alpha * targ_zm).pow(2).sum(-1)\n",
    "            sisnr = 10 * torch.log10((s + eps) / (n + eps))\n",
    "\n",
    "            # Raw RMS power ratio — consistent with power_reg in loss\n",
    "            # raw signal instead of zero-mean so it matches NoiseReductionPct\n",
    "            pred_rms = pred_sq.pow(2).mean(dim=-1).sqrt()  # per sample\n",
    "            targ_rms = targ_sq.pow(2).mean(dim=-1).sqrt()  # per sample\n",
    "            per_sample_ratio = (pred_rms / (targ_rms + eps)).tolist()\n",
    "            mean_ratio = (pred_rms / (targ_rms + eps)).mean().item()\n",
    "\n",
    "            # survives resume correctly\n",
    "            epoch_num = len(self.learn.recorder.values)\n",
    "\n",
    "            samples = sisnr.tolist()\n",
    "            while len(samples) < 4:\n",
    "                samples.append('')\n",
    "\n",
    "            with open(self.fname, 'a', newline='') as f:\n",
    "                csv.writer(f).writerow([\n",
    "                    epoch_num,\n",
    "                    f\"{samples[0]:.4f}\" if samples[0] != '' else '',\n",
    "                    f\"{samples[1]:.4f}\" if samples[1] != '' else '',\n",
    "                    f\"{samples[2]:.4f}\" if samples[2] != '' else '',\n",
    "                    f\"{samples[3]:.4f}\" if samples[3] != '' else '',\n",
    "                    f\"{sisnr.mean().item():.4f}\",\n",
    "                    f\"{pred_sq.min().item():.4f}\",\n",
    "                    f\"{pred_sq.max().item():.4f}\",\n",
    "                    f\"{pred_rms.mean().item():.4f}\",\n",
    "                    f\"{targ_rms.mean().item():.4f}\",\n",
    "                    f\"{mean_ratio:.4f}\"\n",
    "                ])\n",
    "\n",
    "        except Exception as e:\n",
    "            with open(self.fname, 'a', newline='') as f:\n",
    "                csv.writer(f).writerow([\n",
    "                    getattr(self, 'epoch', '?') + 1,\n",
    "                    f\"ERROR: {e}\",\n",
    "                    '', '', '', '', '', '', '', '', ''\n",
    "                ])\n",
    "        finally:\n",
    "            self.learn.model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7687cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PeriodicPESQSTOI(Callback):\n",
    "    \"\"\"\n",
    "    Computes PESQ and STOI on CPU every N epochs on a small validation subset.\n",
    "    Never crashes training — all errors are caught and logged.\n",
    "    \n",
    "    PESQ target: > 2.5\n",
    "    STOI target: > 0.88\n",
    "    \"\"\"\n",
    "    order = 70  # after EpochTracker(60) and Recorder(50)\n",
    "\n",
    "    def __init__(self, fname='training_stats/pesq_stoi_log.csv',\n",
    "                 every_n=10, n_batches=10):\n",
    "        \"\"\"\n",
    "        every_n:   compute every N epochs\n",
    "        n_batches: number of validation batches to evaluate\n",
    "                   10 batches x bs=4 = 40 samples, ~2-3 min on CPU\n",
    "        \"\"\"\n",
    "        self.fname = fname\n",
    "        self.every_n = every_n\n",
    "        self.n_batches = n_batches\n",
    "        self._init_file()\n",
    "\n",
    "    def _init_file(self):\n",
    "        os.makedirs(os.path.dirname(self.fname), exist_ok=True)\n",
    "        if not Path(self.fname).exists():\n",
    "            with open(self.fname, 'w', newline='') as f:\n",
    "                csv.writer(f).writerow([\n",
    "                    'epoch',\n",
    "                    'pesq_mean', 'pesq_min', 'pesq_max',\n",
    "                    'stoi_mean', 'stoi_min', 'stoi_max',\n",
    "                    'n_samples', 'elapsed_seconds'\n",
    "                ])\n",
    "\n",
    "    def after_epoch(self):\n",
    "        # self.epoch is 0-based, so epoch 10 = self.epoch 9\n",
    "        # Use recorder length to get true epoch number (survives resume)\n",
    "        epoch_num = len(self.learn.recorder.values)\n",
    "\n",
    "        if epoch_num % self.every_n != 0:\n",
    "            return\n",
    "\n",
    "        t_start = time.time()\n",
    "\n",
    "        try:\n",
    "            from pesq import pesq as pesq_fn\n",
    "            from pystoi import stoi as stoi_fn\n",
    "        except ImportError as e:\n",
    "            print(f\"\\n[PeriodicPESQSTOI] Import error: {e} — skipping\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            self.learn.model.eval()\n",
    "\n",
    "            pesq_scores = []\n",
    "            stoi_scores = []\n",
    "\n",
    "            for batch_idx, (xb, yb) in enumerate(self.learn.dls.valid):\n",
    "                if batch_idx >= self.n_batches:\n",
    "                    break\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    pred = self.learn.model(xb)\n",
    "\n",
    "                # Move to CPU float — avoid DirectML precision issues\n",
    "                pred_np = pred.squeeze(1).detach().float().cpu().numpy()\n",
    "                targ_np = yb.squeeze(1).detach().float().cpu().numpy()\n",
    "\n",
    "                for i in range(len(pred_np)):\n",
    "                    # PESQ\n",
    "                    try:\n",
    "                        score = pesq_fn(16000, targ_np[i], pred_np[i], 'wb')\n",
    "                        pesq_scores.append(score)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "                    # STOI\n",
    "                    try:\n",
    "                        score = stoi_fn(targ_np[i], pred_np[i],\n",
    "                                        16000, extended=False)\n",
    "                        stoi_scores.append(score)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "            elapsed = time.time() - t_start\n",
    "\n",
    "            pesq_mean = float(np.mean(pesq_scores)) if pesq_scores else 0.0\n",
    "            pesq_min  = float(np.min(pesq_scores))  if pesq_scores else 0.0\n",
    "            pesq_max  = float(np.max(pesq_scores))  if pesq_scores else 0.0\n",
    "            stoi_mean = float(np.mean(stoi_scores)) if stoi_scores else 0.0\n",
    "            stoi_min  = float(np.min(stoi_scores))  if stoi_scores else 0.0\n",
    "            stoi_max  = float(np.max(stoi_scores))  if stoi_scores else 0.0\n",
    "            n_samples = len(pesq_scores)\n",
    "\n",
    "            # Print summary\n",
    "            print(f\"\\n[Epoch {epoch_num}] PESQ={pesq_mean:.4f} \"\n",
    "                  f\"(min={pesq_min:.4f} max={pesq_max:.4f}) | \"\n",
    "                  f\"STOI={stoi_mean:.4f} \"\n",
    "                  f\"(min={stoi_min:.4f} max={stoi_max:.4f}) | \"\n",
    "                  f\"n={n_samples} | {elapsed:.1f}s\")\n",
    "\n",
    "            # Log targets\n",
    "            pesq_target = \"✓\" if pesq_mean > 2.5  else \"✗\"\n",
    "            stoi_target = \"✓\" if stoi_mean > 0.88 else \"✗\"\n",
    "            print(f\"           PESQ target >2.5:  {pesq_target} | \"\n",
    "                  f\"STOI target >0.88: {stoi_target}\")\n",
    "\n",
    "            with open(self.fname, 'a', newline='') as f:\n",
    "                csv.writer(f).writerow([\n",
    "                    epoch_num,\n",
    "                    f\"{pesq_mean:.4f}\", f\"{pesq_min:.4f}\", f\"{pesq_max:.4f}\",\n",
    "                    f\"{stoi_mean:.4f}\", f\"{stoi_min:.4f}\", f\"{stoi_max:.4f}\",\n",
    "                    n_samples,\n",
    "                    f\"{elapsed:.1f}\"\n",
    "                ])\n",
    "\n",
    "        except Exception as e:\n",
    "            elapsed = time.time() - t_start\n",
    "            print(f\"\\n[PeriodicPESQSTOI] Error at epoch {epoch_num}: {e}\")\n",
    "            with open(self.fname, 'a', newline='') as f:\n",
    "                csv.writer(f).writerow([\n",
    "                    epoch_num,\n",
    "                    f\"ERROR: {e}\",\n",
    "                    '', '', '', '', '', '', f\"{elapsed:.1f}\"\n",
    "                ])\n",
    "\n",
    "        finally:\n",
    "            self.learn.model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17017ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_epoch_seed(epoch):\n",
    "    \"\"\"Call before each epoch from main process\"\"\"\n",
    "    EPOCH_SEED.value = epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bebe45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_raw_np(path, target_sr=16000):\n",
    "    \"\"\"Load audio file → mono float32 numpy [T]\"\"\"\n",
    "    wave, sr = torchaudio.load(str(path))\n",
    "    if wave.shape[0] > 1:\n",
    "        wave = wave.mean(0, keepdim=True)\n",
    "    if sr != target_sr:\n",
    "        wave = torchaudio.transforms.Resample(sr, target_sr)(wave)\n",
    "    return wave.squeeze(0).numpy().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370a060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_strategy(path, epoch_seed):\n",
    "    \"\"\"\n",
    "    Deterministic strategy selection per file per epoch.\n",
    "    Same path + epoch → same strategy always.\n",
    "    Changes every epoch for augmentation diversity.\n",
    "    \"\"\"\n",
    "    h = abs(hash((str(path), int(epoch_seed)))) % 1000\n",
    "    if h < 500:\n",
    "        return 'denoise_only'   # 50%\n",
    "    elif h < 800:\n",
    "        return 'joint'          # 30%\n",
    "    else:\n",
    "        return 'deverb_only'    # 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1416570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_reverb_rng(noisy_path, epoch_seed):\n",
    "    \"\"\"\n",
    "    Deterministic RNG for reverb simulation.\n",
    "    Same path + epoch → same room dimensions → same RIR for get_x and get_y.\n",
    "    \"\"\"\n",
    "    seed = abs(hash((str(noisy_path), int(epoch_seed), 'rir'))) % (2**31)\n",
    "    return np.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5ca253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_reverb(wave_np, sr=16000, rt60_range=(0.2, 0.6), rng=None):\n",
    "    \"\"\"\n",
    "    Add synthetic room reverb to float32 numpy [T].\n",
    "    rng: numpy RandomState for deterministic output, None for random.\n",
    "    Raises on failure — caller handles fallback.\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.RandomState()\n",
    "\n",
    "    orig_rms = np.sqrt(np.mean(wave_np ** 2)) + 1e-8\n",
    "    rt60     = rng.uniform(*rt60_range)\n",
    "    room_dim = [\n",
    "        rng.uniform(3.0, 8.0),\n",
    "        rng.uniform(3.0, 6.0),\n",
    "        rng.uniform(2.5, 3.5)\n",
    "    ]\n",
    "\n",
    "    e_absorption, max_order = pra.inverse_sabine(rt60, room_dim)\n",
    "    max_order = min(max_order, 12)\n",
    "\n",
    "    room = pra.ShoeBox(\n",
    "        room_dim, fs=sr,\n",
    "        materials=pra.Material(e_absorption),\n",
    "        max_order=max_order\n",
    "    )\n",
    "\n",
    "    def rand_pos(dims):\n",
    "        return [rng.uniform(0.5, d - 0.5) for d in dims]\n",
    "\n",
    "    room.add_source(rand_pos(room_dim), signal=wave_np)\n",
    "    room.add_microphone(np.array(rand_pos(room_dim)).reshape(3, 1))\n",
    "    room.simulate()\n",
    "\n",
    "    out = room.mic_array.signals[0].astype(np.float32)\n",
    "    out = out[:len(wave_np)]\n",
    "    if len(out) < len(wave_np):\n",
    "        out = np.pad(out, (0, len(wave_np) - len(out)))\n",
    "\n",
    "    out_rms = np.sqrt(np.mean(out ** 2)) + 1e-8\n",
    "    return out * (orig_rms / out_rms)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
